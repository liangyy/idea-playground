{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue of EM algorithm\n",
    "\n",
    "The lack of analytical maximizer in M-step makes the EM algorithm pretty nasty. For instance, when it does not converge, it is hard to say whether it is because M-step fails or a bug in EM. A better solution is to use existing solver for M-step but I failed to find a one. \n",
    "\n",
    "**I am also waiting for an analytical result!**\n",
    "\n",
    "### Using `lme4::lmer`\n",
    "\n",
    "The straightforward idea is to use an out-of-box random effect solver. Well, the M-step problem can be formalized as a weighted random effect model (for $K = 2$).\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\hat\\beta_{gwas} &= \\beta_{gene, 1} \\hat\\beta_{eqtl, 1} + \\beta_{gene, 2} \\hat\\beta_{eqtl, 2} + e \\\\\n",
    "    \\hat\\beta_{eqtl, 1}, \\hat\\beta_{eqtl, 2} &= \\begin{cases}\n",
    "        \\hat\\beta_{eqtl}, 0 \\\\\n",
    "        0, \\hat\\beta_{eqtl}\n",
    "    \\end{cases} \\\\\n",
    "    \\beta_{gene, 1} &\\sim \\mathcal{N}(0, \\sigma_1^2) \\\\\n",
    "    \\beta_{gene, 2} &\\sim \\mathcal{N}(0, \\sigma_2^2) \\\\\n",
    "    e &\\sim \\mathcal{N}(0, \\sigma^2)\n",
    "\\end{align*}$$\n",
    "\n",
    "By doing this construction, it is almost solving the M-step problem and the only missing part is weight. Let $y = [\\hat\\beta_{gwas}, \\hat\\beta_{gwas}]$, $x_1 = [\\hat\\beta_{eqtl}, \\vec{0}], x_2 = [\\vec{0}, \\hat\\beta_{eqtl}], w = [w_{11}, \\cdots, w_{n1}, w_{12}, \\cdots, w_{n2}]$. The equivalent random effect model is\n",
    "\n",
    "$$\\begin{align*}\n",
    "    y &= \\beta_{gwas, 1} x_1 + \\beta_{gwas, 2} x_2 + e \n",
    "\\end{align*}$$\n",
    ", with weighted likelihood \n",
    "$$\\begin{align*}\n",
    "    lld &= \\sum_i \\log w_i \\Pr(y_i | x_1, x_2)\n",
    "\\end{align*}$$\n",
    "\n",
    "Unfortunately, I find no way to use weighted likelihood in `lme4::lmer`. The `weights` option in it is for modeling heteroscedasticity. Nonetheless, `lme4::lmer` becomes a validation of my own M-step solver by applying to the case with equal weights. See [`compare_to_lmer.html`](http://htmlpreview.github.io/?https://raw.githubusercontent.com/liangyy/idea-playground/master/mix_random_effect/compare_to_lmer.html). \n",
    "\n",
    "**I am waiting for a better alternative RE solver!**\n",
    "\n",
    "\n",
    "### EM solver performance\n",
    "\n",
    "See [`run.html`](http://htmlpreview.github.io/?https://raw.githubusercontent.com/liangyy/idea-playground/master/mix_random_effect/run.html). Note that `idx` determines the initial value of EM algorithm. I generated 3 data sets with sample sizes 1e3, 1e4, 1e5. EM was run 10 times with 10 initial points. The solution is not satisfiable since it is not very stable across runs and the one with highest likelihood is not quite close to the truth. $\\hat\\sigma^2$ is fine, but $\\hat\\sigma_2^2$ (the bigger one) is quite problematic. And the worst thing is that $\\hat\\pi$ is not really trustable, which is kinda the only variable we really care about. So that it makes the whole framework seemingly useless.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
