{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n",
    "\n",
    "We would like to examine the distribution of the effect of gene expression ($\\beta_{gene}$) on trait across all genes. First of all, let's ignore the possibility that a variant can also have un-mediated effect. \n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\hat\\beta_{gwas} &= \\beta_{gene} \\hat\\beta_{eqtl} + \\epsilon \\\\\n",
    "    \\epsilon &\\sim \\mathcal{N}(0, \\sigma^2) \\\\\n",
    "    \\beta_{gene} &\\sim g\n",
    "\\end{align*}$$\n",
    "\n",
    "Ideally, we want to estimate $g$ without any assumption on the shape of $g$ (non-parametric method) but let's keep anything simple here and assume that $g$ is a mixture of Gaussian. Namely,\n",
    "\n",
    "$$\\begin{align*}\n",
    "    g = \\sum_{k = 1}^K \\pi_k \\mathcal{N}(0, \\sigma_k^2)\n",
    "\\end{align*}$$\n",
    "\n",
    "Then, the problem of solving for $g$ becomes to estimate $\\pi_k, \\sigma_k^2$ from data $\\hat\\beta_{gwas}, \\hat\\beta_{eqtl}$. \n",
    "\n",
    "## EM algorithm\n",
    "\n",
    "For a simple random effect model, the likelihood is obtained by marignalizing out $\\beta_{gene}$. Namely, (for $K = 1$ case)\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\beta_{gwas} &\\sim \\mathcal{N}(0, \\hat\\beta_{eqtl}^2 \\sigma_1^2 + \\sigma^2)\n",
    "\\end{align*}$$\n",
    "\n",
    "So that, by introducing the hidden variable $Z$ indicating which component is really contributing to the variance term, the complete likelihood is\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\beta_{gwas}| Z = k &\\sim \\mathcal{N}(0, \\hat\\beta_{eqtl}^2 \\sigma_k^2 + \\sigma^2) \\\\\n",
    "    Z &\\sim Multinomial(1; \\pi_1, \\dots, \\pi_k)\n",
    "\\end{align*}$$\n",
    "\n",
    "From the complete likelihood, we can obtain the EM algorithm. \n",
    "\n",
    "- **E-step**:\n",
    "\n",
    "$$\\begin{align*}\n",
    "    L_{ki} &= \\mathcal{N}(\\hat\\beta_{gwas, i}; 0, \\hat\\beta_{eqtl, i}^2 \\sigma_k^2 + \\sigma^2) \\\\\n",
    "    w_{ki}^{(t)} &= \\Pr(Z_i = k | Data_i, \\theta^{(t)}) \\\\\n",
    "    &= \\frac{L_{ki}^{(t)} \\pi_k^{(t)}}{\\sum_{k'} L_{k'i}^{(t)} \\pi_{k'}^{(t)}}\n",
    "\\end{align*}$$\n",
    "\n",
    "- **M-step**:\n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\pi_{k}^{(t+1)} &= \\frac{\\sum_i {w_{ki}^{(t)}}}{\\sum_{k'} \\sum_i {w_{k'i}^{(t)}}} \\\\\n",
    "    \\sigma_k^{(t+1)}, \\sigma^{(t+1)} &= \\arg\\max_{\\sigma_k, \\sigma} \\sum_k \\sum_i w_{ki}^{(t)} (\\frac{1}{2}\\log\\frac{1}{\\hat\\beta_{eqtl, i}^2\\sigma_k^2 + \\sigma^2} - \\frac{1}{2} \\frac{\\hat\\beta_{gwas, i}^2}{\\hat\\beta_{eqtl, i}^2\\sigma_k^2 + \\sigma^2})\n",
    "\\end{align*}$$\n",
    "\n",
    "To solve for the last optimization problem, I plan to use gradient decsent as first try. To make it unconstraint ($\\sigma, \\sigma_k$ should be at least non-zero), let $x = \\log(\\sigma^2), x_k = \\log(\\sigma_k^2)$. \n",
    "\n",
    "$$\\begin{align*}\n",
    "    \\tau_{ki} &= \\hat\\beta_{eqtl, i}^{2} \\sigma_k^{2} + \\sigma^{2} \\\\\n",
    "    f_1 &= -\\frac{1}{\\tau_{ki}} + \\frac{\\hat\\beta_{gwas, i}^2}{\\tau_{ki}^2} \\\\\n",
    "    \\frac{\\partial}{\\partial x_k} &= e^{x_k} \\sum_i \\hat\\beta_{eqtl, i}^{2} w_{ki}^{(t)} f_1 \\\\\n",
    "    \\frac{\\partial}{\\partial x} &= e^{x} \\sum_i \\sum_k w_{ki}^{(t)} f_1\n",
    "\\end{align*}$$\n",
    "\n",
    "It turns out that gradient descent solution is not close enough to the optimum (in term of the fact the gradient is far from zero). I also used Newton-Raphson's method to improve the solution locally. \n",
    "\n",
    "$$\\begin{align*}\n",
    "    f_2 &= \\frac{1}{\\tau_{ki}^2} - \\frac{2\\hat\\beta_{gwas, i}^2}{\\tau_{ki}^3} \\\\\n",
    "    \\frac{\\partial^2}{\\partial x_k \\partial x_k} &= e^{x_k} \\sum_i \\hat\\beta_{eqtl, i}^{2} w_{ki}^{(t)} f_1 + e^{x_k} \\sum_i \\beta_{eqtl, i}^{2} w_{ki}^{(t)} f_2\\\\\n",
    "    \\frac{\\partial^2}{\\partial x \\partial x} &= e^{x} \\sum_i \\sum_k w_{ki}^{(t)} f_1 + e^{x} \\sum_i \\sum_k w_{ki}^{(t)} f_2 \\\\\n",
    "    \\frac{\\partial^2}{\\partial x \\partial x_k} &= e^{x} \\sum_i \\sum_k \\hat\\beta_{eqtl, i}^2 w_{ki}^{(t)} f_2\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "## Other strategy? Estimating mixture proportion with grid search\n",
    "\n",
    "Another thing I can imagine is to fix $\\sigma_k^2$ in mixture model and estimate mixture proportion. At least in this setting, we can obtain analytical M-step or even I may find out-of-box solver for this task ([`ashr`](https://github.com/stephens999/ashr)?)\n",
    "\n",
    "If I naively replace the above mixture model with fixed $\\sigma_k^2$, the M-step is still lack of analytical result. The only incremental improvement is that the optimization problem (root-finding problem) reduces from $K+1$ dimensions to one dimension. Then I would not bother to implement unless my previous solver is too unstable to rely on. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
